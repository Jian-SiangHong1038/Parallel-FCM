{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spring Simulation\n",
    "## An Ecological Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports for notebook\n",
    "import networkx as nx\n",
    "import math\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import os\n",
    "from FCM import FCM\n",
    "from FCM import simulation\n",
    "from collections import defaultdict\n",
    "import skfuzzy as fuzz\n",
    "import pandas\n",
    "from skfuzzy import control as ctrl\n",
    "from scipy.stats.stats import pearsonr  \n",
    "from scipy.stats import kendalltau\n",
    "from __future__ import division\n",
    "import operator\n",
    "import time\n",
    "from sklearn.linear_model import LinearRegression, TheilSenRegressor\n",
    "\n",
    "from sklearn.linear_model import RANSACRegressor\n",
    "from scipy.stats import spearmanr\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#%matplotlib qt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Merging Several Graphs\n",
    "As we will be working with several graphs with different edge weights our first goal is to be able to properly merge the graphs together and deal with merging similar edges. We will merge similar edges by using the average weight amongst all graphs in a set, though other methods to deal with similar edges can be provided as functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "average\n",
    "parameters: dictionary of edges and their weights\n",
    "returns: a dictionary with (edge,value)\n",
    "Desc: This function will average the weight on the edges when we merge graphs\n",
    "'''\n",
    "def average(weightDict):\n",
    "    \n",
    "    for key in weightDict:\n",
    "        weightDict[key] = (sum(weightDict[key])/len(weightDict[key]))\n",
    "   \n",
    "    return weightDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "mergeGrapgList\n",
    "takes 2 arguments\n",
    "graphList: a list of weighted digraphs\n",
    "f: the function for dealing with same edges\n",
    "returns: the merged weighted digraph \n",
    "merge all weighted digraaphs in a list using provided function f to deal with edge collisions. \n",
    "'''\n",
    "def mergeGraphList(graphList, f):\n",
    "    \n",
    "    mergedGraph = nx.DiGraph()\n",
    "    weights = defaultdict(list)\n",
    "    \n",
    "    for graph in graphList:\n",
    "        for edge in graph.edges_iter():\n",
    "            #save the edge weight and add the edge to the graph. If it already there the weight may be overwritten but we will fix at end\n",
    "            x=graph.get_edge_data(*edge)['weight'] #get weight from edge and save in list of that edges weights\n",
    "            weights[edge].append(x)\n",
    "            #add edge to merged graph. Note that if the nodes don't exist add edge cretes them\n",
    "            mergedGraph.add_edge(*edge)\n",
    "                \n",
    "    \n",
    "    #once the loop is over we should have a dictionary of lists for all weights on that edge. We will pass that dictionary\n",
    "    #to our average function to get the average weight for each edge\n",
    "    weightsAvg = f(weights)\n",
    "    \n",
    "    #update edges with new weights\n",
    "    for key in weightsAvg:\n",
    "        mergedGraph.add_edge(key[0],key[1],weight=weightsAvg[key])\n",
    "        \n",
    "    #return merged graph\n",
    "    return mergedGraph\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Test merge function actually merges  graphs\n",
    "Dunction will initialize 3 graphs and merge them together. The edges and their values will be printed out\n",
    "to show the merged result. We apply the average function above for overlapping edges. At the end we print out all three merged\n",
    "graphs\n",
    "'''\n",
    "def testMerge():\n",
    "    \n",
    "    G=nx.DiGraph()\n",
    "    G.add_edge('a','b',{'weight':1})\n",
    "    G.add_edge('a','c',{'weight':2})\n",
    "    G.add_edge('c','d',{'weight':3})\n",
    "\n",
    "    G1 = nx.DiGraph()\n",
    "    G1.add_edge('b','d',{'weight' : 4})\n",
    "    G1.add_edge('e','b',{'weight' : 5})\n",
    "#initialize third graph\n",
    "    G2 = nx.DiGraph()\n",
    "    G2.add_edge('a','b',{'weight' : 1})\n",
    "    G2.add_edge('e','b',{'weight' : 10})\n",
    "    G2.add_edge('c','d',{'weight' : 5})\n",
    "    G2.add_edge('e','c',{'weight' : 3})\n",
    "    \n",
    "    print \"First we will show a merge with no similar edges\"\n",
    "    print \"G edges:\", G.edges()\n",
    "    print\n",
    "    print\n",
    "    print \"G1 edges: \", G1.edges()\n",
    "    print\n",
    "    print\n",
    "    g3 = mergeGraphList([G1,G],average)\n",
    "    nx.draw(g3,pos=nx.spring_layout(g3),with_labels=True,with_attributes=True)\n",
    "    print \"Combined edges: \", g3.edges()\n",
    "\n",
    "    x=G.edges()\n",
    "    y=G1.edges()\n",
    "    for edge in g3.edges():\n",
    "        print \"Edge is: \", edge, \" with weight: \", g3.edge[edge[0]][edge[1]]['weight']\n",
    "    z=g3.edges()\n",
    "\n",
    "    missingEdge = False\n",
    "\n",
    "    for edge in x:\n",
    "        if edge not in z:\n",
    "            print \"Edge from graph one missing in merge: \", edge\n",
    "            missingEdge = True\n",
    "        \n",
    "    for edge in y:\n",
    "        if edge not in z:\n",
    "            print \"Edge from graph 2 missing in merge: \", edge\n",
    "            missingEdge = True\n",
    "        \n",
    "    if missingEdge:\n",
    "        print \"Not all edges merged\"\n",
    "        \n",
    "    else:\n",
    "        print \"Graphs merged successfully\"\n",
    "\n",
    "\n",
    "    print \"First we will show a merge with no similar edges\"\n",
    "    print \"G edges:\", G.edges()\n",
    "    print\n",
    "    print\n",
    "    print \"G1 edges: \", G1.edges()\n",
    "    print\n",
    "    print\n",
    "    print \"G2 edges: \", G2.edges()\n",
    "    g3 = mergeGraphList([G,G1,G2],average)\n",
    "    nx.draw(g3,pos=nx.spring_layout(g3),with_labels=True,with_attributes=True)\n",
    "    print \"Combined edges: \", g3.edges()\n",
    "\n",
    "    x=G.edges()\n",
    "    y=G1.edges()\n",
    "    w = G2.edges()\n",
    "    for edge in g3.edges():\n",
    "        print \"Edge is: \", edge, \" with weight: \", g3.edge[edge[0]][edge[1]]['weight']\n",
    "    z=g3.edges()\n",
    "\n",
    "    missingEdge = False\n",
    "\n",
    "    for edge in x:\n",
    "        if edge not in z:\n",
    "            print \"Edge from graph one missing in merge: \", edge\n",
    "            missingEdge = True\n",
    "        \n",
    "    for edge in y:\n",
    "        if edge not in z:\n",
    "            print \"Edge from graph 2 missing in merge: \", edge\n",
    "            missingEdge = True\n",
    "        \n",
    "    for edge in w:\n",
    "        if edge not in z:\n",
    "            print \"Edge from graph 2 missing in merge: \", edge\n",
    "            missingEdge = True        \n",
    "\n",
    "    if missingEdge:\n",
    "        print \"Not all edges merged\"\n",
    "        \n",
    "    else:\n",
    "        print \"Graphs merged successfully\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have shown that we can merge 2 graphs lets add a third to the equation with overlapping edges. In the graph declaed below we create 4 edges. The first (a,b) edge has the same weight as the edge already there, thus we expect the weight to remain the same at 1. The second edge (e,b) has an added weight of 10 so we expect the new weight of 7.5. The third edge (c,d), has a new weight of 4 which means we can expect an average a 3 since the original weight is 2. Finally we add a new edge independent of previous ones.\n",
    "\n",
    "From the above output we can see that all three graphs have successfully merged together and the weights have averaged properly to their expected values.\n",
    "\n",
    "Through observation it can be seen the original graphs share two edges (a,b) and (c,e) and the weight of their new merged edge is the average of the two oroginal edges, meaning that the graphs merge properly which means:\n",
    "a) All edges and nodes from both graphs are included in the new graph\n",
    "b) If both graphs have the same edge, the weight of the edge will be the average of the two, or whatever function is passed for the merge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Graph Extraction\n",
    "We have many graphs from our different groups of interest, anglers, club managers, water manaagers and experts. Using our data we will create the graphs from the file using networkx and then place the graphs for each type into a list of graphs to form a group, for example all angler graphs would be in one list. To make sure the proper number of graphs was created for each section we will get a count of the number of files in a group and make sure that count matches the number of graphs in a given group.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "createGraphList\n",
    "reads all .csv files in the input subdirectory\n",
    "if a .csv file name matches one in the input fileNameList\n",
    "converts the file into a numpy matrix and then to a networkx digraph, and adds it to a list\n",
    "if there is an error, prints the name of the file instead\n",
    "expects the path of a subdirectory from the current working directory as input\n",
    "***this path must end in \"\\\\\" to escape the last backslash***\n",
    "returns the list of graphs\n",
    ".csv files are expected to be properly cleaned, containing only the adjacenecy matrix with no labels\n",
    "'''\n",
    "def createGraphList(subdirectory, fileNameList):\n",
    "    graphList = []\n",
    "    \n",
    "    #get the current working directory and append the input subdirectory to it\n",
    "    directory = os.getcwd()\n",
    "    directory += subdirectory\n",
    "    \n",
    "    #for each file in the specified directory\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            #if the file is a .csv file\n",
    "            if file.endswith(\".csv\"):\n",
    "                #if the file name appears in the fileNameList\n",
    "                if file in fileNameList:\n",
    "                    #try to read the adjacency matrix in the file, convert it into a graph, and add that graph to a list\n",
    "                    #if there are non-numeric values, throw exception and print the file name\n",
    "                    try:\n",
    "                        mapMatrix = np.loadtxt(open(directory + file,\"rb\"), dtype = float, delimiter = \",\")\n",
    "                        mapGraph = nx.DiGraph(mapMatrix)                  \n",
    "                        graphList.append(mapGraph)\n",
    "                    except ValueError:\n",
    "                        print (file)\n",
    "    \n",
    "    return graphList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "createFileList\n",
    "parameters: subdirectory: path to directory where files are held\n",
    "returns: the list of files where map are stored\n",
    "Will access the directories for each group\n",
    "grab the name of each file and place them into a list that is returned\n",
    "'''\n",
    "def createFileList(subdirectory):\n",
    "    fileList = []\n",
    "    \n",
    "    #get the current working directory and append the input subdirectory to it\n",
    "    directory = os.getcwd()\n",
    "    directory += subdirectory\n",
    "    \n",
    "    #for each file in the specified directory\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            #if the file is a .csv file\n",
    "            if file.endswith(\".csv\"):\n",
    "                #add the file name to a list\n",
    "                fileList.append(file)\n",
    "    \n",
    "    return fileList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "createFileLists\n",
    "access the folders that hold the files of all the mental maps. Extract th ids and the graphs for each one\n",
    "returns the id lists and graph lists for each group\n",
    "\n",
    "example:\n",
    "#intialize the file lists\n",
    "anglerIDList, clubManagerIDList, waterManagerIDList, expertIDList, nonExpertIDList, anglerGraphList, clubManagerGraphList, waterManagerGraphList,expertGraphList,nonExpertGraphList = createFileLists()\n",
    "'''\n",
    "def createFileLists():\n",
    "\n",
    "    anglerIDList = createFileList(\"/Angler_Final//\")\n",
    "    clubManagerIDList = createFileList(\"/Club_Managers_Final//\")\n",
    "    waterManagerIDList = createFileList(\"/Water_Managers_Final//\")\n",
    "    expertIDList = createFileList(\"/Expert Maps//\")\n",
    "    nonExpertIDList = createFileList(\"/NonExperts_Final//\")\n",
    "\n",
    "#create graph lists for each group Files must be in proper areas for access. As these are paths make sure to have proper paths\n",
    "    anglerGraphList = createGraphList(\"/NonExperts_Final//\", anglerIDList)\n",
    "    clubManagerGraphList = createGraphList(\"/Club_Managers_Final//\", clubManagerIDList)\n",
    "    waterManagerGraphList = createGraphList(\"/NonExperts_Final//\", waterManagerIDList)\n",
    "    expertGraphList = createGraphList(\"/Expert Maps//\", expertIDList)\n",
    "    nonExpertGraphList = createGraphList(\"/NonExperts_Final//\", nonExpertIDList)\n",
    "    \n",
    "    return anglerIDList, clubManagerIDList, waterManagerIDList, expertIDList, nonExpertIDList, anglerGraphList, clubManagerGraphList, waterManagerGraphList,expertGraphList,nonExpertGraphList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "compares the length of the two input lists\n",
    "outputs a message that they are the same\n",
    "or a message with the difference in their lengths\n",
    "'''\n",
    "def compareListLength(list1, list2):\n",
    "    if(len(list1) == len(list2)):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "checkListSizes\n",
    "arguments: id lists and graphs lists for each group\n",
    "returns: null\n",
    "will compare to see if the number of id's extracted matches the number of graphs to make sure we have the proper \n",
    "number of files\n",
    "\n",
    "example:\n",
    "checkListSizes(anglerIDList, clubManagerIDList, waterManagerIDList, expertIDList, nonExpertIDList, anglerGraphList, clubManagerGraphList, waterManagerGraphList,expertGraphList,nonExpertGraphList)\n",
    "'''\n",
    "def checkListSizes(anglerIDList, clubManagerIDList, waterManagerIDList, expertIDList, nonExpertIDList, anglerGraphList, clubManagerGraphList, waterManagerGraphList,expertGraphList,nonExpertGraphList):\n",
    "#test the sizes for maps to make sure all created\n",
    "    if compareListLength(anglerIDList, anglerGraphList):\n",
    "        print \"Angler size match\"\n",
    "    else:\n",
    "        print \"Angler failure\"\n",
    "    \n",
    "    if compareListLength(clubManagerIDList, clubManagerGraphList):\n",
    "        print \"Club Manager match\"\n",
    "    else:\n",
    "        print \"Club Manager failure\"\n",
    "    \n",
    "    if compareListLength(waterManagerIDList, waterManagerGraphList):\n",
    "        print \"Water Manager Success\"\n",
    "    else:\n",
    "        print \"Water Manager Failure\"\n",
    "    \n",
    "    if compareListLength(expertIDList, expertGraphList):\n",
    "        print \"Expert match\"\n",
    "    else:\n",
    "        print \"Expert Failure\"\n",
    "    \n",
    "    if compareListLength(nonExpertIDList, nonExpertGraphList):\n",
    "        print \"Non-Expert Aggregate Success\"\n",
    "    else:\n",
    "        print \"Non-Expert Aggregate Failure\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from above that we create an amount of graphs created matches the number of files for each group. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "combineGraphLists \n",
    "returns: Combined graphs\n",
    "will merge the graphs inth e grah lists using the passed in function for overlapping edges\n",
    "\n",
    "example:\n",
    "anglerCombined,clubManagerCombined,waterManagerCombined,expertCombined,nonExpertCombined = combineGraphLists(anglerGraphList,clubManagerGraphList,waterManagerGraphList,expertGraphList,nonExpertGraphList,average)\n",
    "'''\n",
    "def combineGraphLists(anglerGraphList,clubManagerGraphList,waterManagerGraphList,expertGraphList,nonExpertGraphList,average):\n",
    "    anglerCombined = mergeGraphList(anglerGraphList,average)\n",
    "    clubManagerCombined = mergeGraphList(clubManagerGraphList,average)\n",
    "    waterManagerCombined = mergeGraphList(waterManagerGraphList,average)\n",
    "    expertCombined = mergeGraphList(expertGraphList,average)\n",
    "    nonExpertCombined = mergeGraphList(nonExpertGraphList,average)\n",
    "\n",
    "    return anglerCombined,clubManagerCombined,waterManagerCombined,expertCombined,nonExpertCombined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Creating Fuzzy Cognitive Maps\n",
    "\n",
    "For our data set he individuals rated each connection on strength and whether it was positive or negative. To get a concrete value we will use fuzzy logic. We will use 6 triangular membership functions for the values of $\\pm 0.25, \\pm 0.5, and \\pm 0.75$. For 0.25 it starts at 0, peaks at 0.25, goes down to 0.5; for 0.5 it starts at 0.25, peaks at 0.5, goes down to 0.75; for 0.75 it starts at 0.5, peaks at 0.75, and goes down to 1. The negative version looks the same. TO create our Fuzzy cognitive maps (FCM) we will used the attached python library FCM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "mamdani_rule\n",
    "parameters: a: array of integers \n",
    "            b: array of integers\n",
    "            weight: ratio of concept\n",
    "returns: A quantitative value for the edge from qualitative information\n",
    "implement mamdani rule for deffuzification\n",
    "'''\n",
    "def mamdani_rule(a,b, weight):\n",
    "    m = len(a)\n",
    "    n = len(b)\n",
    "    a = np.atleast_2d(a)\n",
    "    b = np.atleast_2d(b)\n",
    "    \n",
    "    return np.fmax(np.dot(a.T, np.ones((1,n))), \n",
    "                   np.dot(np.multiply(np.ones((m,1)),weight),\n",
    "                   b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "makeFuzzy\n",
    "parameters: the mental models\n",
    "returns: An FCM\n",
    "takes in mental models and using fuzzy math creates the FCMs\n",
    "'''\n",
    "def makeFuzzy(maps):\n",
    "    new_fcm = FCM()\n",
    "    \n",
    "    # Add concepts \n",
    "    new_fcm.add_concept('pike population (adult, over the legal size limit)')\n",
    "    new_fcm.add_concept('stocked pike (adult, over the legal size limit)')\n",
    "    new_fcm.add_concept('stocked pike, young fish (under the legal size limit)')\n",
    "    new_fcm.add_concept('baitfish, prey fish')\n",
    "    new_fcm.add_concept('other predatory fish')\n",
    "    new_fcm.add_concept('algae')\n",
    "    new_fcm.add_concept('depth of a body of water')\n",
    "    new_fcm.add_concept('spawning grounds')\n",
    "    new_fcm.add_concept('wild pike, young fish (under the legal size limit)')\n",
    "    new_fcm.add_concept('emergent riparian plants (eg reeds and other bank vegetation)')\n",
    "    new_fcm.add_concept('benthic invertebrates (snails, crustaceans etc)')\n",
    "    new_fcm.add_concept('zooplankton')\n",
    "    new_fcm.add_concept('submerged aquatic plants')\n",
    "    new_fcm.add_concept('cormorant')\n",
    "    new_fcm.add_concept('plant nutrients')\n",
    "    new_fcm.add_concept('turbidity of water')\n",
    "    new_fcm.add_concept('angling pressure')\n",
    "    new_fcm.add_concept('hiding places, refuges')\n",
    "    new_fcm.add_concept('surface area of a body of water')\n",
    "    \n",
    "    conceptList = [\n",
    "                  'pike population (adult, over the legal size limit)',\n",
    "    'stocked pike (adult, over the legal size limit)',\n",
    "    'stocked pike, young fish (under the legal size limit)',\n",
    "    'baitfish, prey fish',\n",
    "    'other predatory fish',\n",
    "    'algae',\n",
    "    'depth of a body of water',\n",
    "    'spawning grounds',\n",
    "    'wild pike, young fish (under the legal size limit)',\n",
    "    'emergent riparian plants (eg reeds and other bank vegetation)',\n",
    "    'benthic invertebrates (snails, crustaceans etc)',\n",
    "    'zooplankton',\n",
    "    'submerged aquatic plants',\n",
    "    'cormorant',\n",
    "    'plant nutrients',\n",
    "    'turbidity of water',\n",
    "    'angling pressure',\n",
    "    'hiding places, refuges',\n",
    "    'surface area of a body of water'\n",
    "    ]\n",
    "    \n",
    "    # Dictionary with keys as 'edges' and value as a list of 'weights'\n",
    "    conceptVals = defaultdict(list)#creates an empty dictionary of (None,list) \n",
    "    for _map in maps:\n",
    "            \n",
    "        # Get all weights\n",
    "        for edge in _map.edges_iter():#iterate through edges\n",
    "            \n",
    "            if conceptVals.has_key(edge):\n",
    "                # Add to existing entry \n",
    "                conceptVals[edge].append(_map.get_edge_data(*edge)['weight'])\n",
    "            else:\n",
    "                conceptVals[edge] = [_map.get_edge_data(*edge)['weight']]\n",
    "    \n",
    "    # Aggregate the maps using fuzzyAggregation\n",
    "    fuzzDictionary = fuzzyAggregation(conceptVals)\n",
    "        \n",
    "    # Add the edges with the new fuzzy values to the FCM\n",
    "    \n",
    "    for edge in fuzzDictionary:\n",
    "        new_fcm.add_edge(conceptList[edge[0]], conceptList[edge[1]], fuzzDictionary[edge])\n",
    "        \n",
    "    new_fcm.set_value('pike population (adult, over the legal size limit)', 1.0)\n",
    "    new_fcm.set_value('stocked pike (adult, over the legal size limit)', 1.0)\n",
    "    new_fcm.set_value('stocked pike, young fish (under the legal size limit)', 1.0)\n",
    "    new_fcm.set_value('baitfish, prey fish', 1.0)\n",
    "    new_fcm.set_value('other predatory fish', 1.0)\n",
    "    new_fcm.set_value('algae', 1.0)\n",
    "    new_fcm.set_value('depth of a body of water', 1.0)\n",
    "    new_fcm.set_value('spawning grounds', 1.0)\n",
    "    new_fcm.set_value('wild pike, young fish (under the legal size limit)', 1.0)\n",
    "    new_fcm.set_value('emergent riparian plants (eg reeds and other bank vegetation)', 1.0)\n",
    "    new_fcm.set_value('benthic invertebrates (snails, crustaceans etc)',1.0)\n",
    "    new_fcm.set_value('zooplankton', 1.0)\n",
    "    new_fcm.set_value('submerged aquatic plants', 1.0)\n",
    "    new_fcm.set_value('cormorant', 1.0)\n",
    "    new_fcm.set_value('plant nutrients', 1.0)\n",
    "    new_fcm.set_value('turbidity of water', 1.0)\n",
    "    new_fcm.set_value('angling pressure', 1.0)\n",
    "    new_fcm.set_value('hiding places, refuges', 1.0)\n",
    "    new_fcm.set_value('surface area of a body of water', 1.0) \n",
    "   \n",
    "    \n",
    "    return new_fcm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "conceptVals -> dictionary with all the values from each edge\n",
    "fuzzyAggregation uses fuzzy logic to aggregate weights into fuzzy values, and returns a dictionary[edge][fuzzVal]\n",
    "'''\n",
    "def fuzzyAggregation(conceptVals):\n",
    "    # universal, goes from -1.05 to 1.05 in 0.05 increments\n",
    "    univ = np.arange(-1.05, 1.05, .05)\n",
    "\n",
    "    # membership functions\n",
    "    p_tri25 = fuzz.trimf(univ, [0, 0.25, 0.5])\n",
    "    p_tri50 = fuzz.trimf(univ, [0.25, 0.5, 0.75])\n",
    "    p_tri75 = fuzz.trimf(univ, [0.5, 0.75, 1])\n",
    "\n",
    "   \n",
    "    n_tri25 = fuzz.trimf(univ, [-0.5, -0.25, 0])\n",
    "    n_tri50 = fuzz.trimf(univ, [-0.75, -0.5, -0.25])\n",
    "    n_tri75 = fuzz.trimf(univ, [-1, -0.75, -0.5])\n",
    "\n",
    "    \n",
    "    r_input = fuzz.trimf(univ, [1.00,1.00,1.00])\n",
    "    aggDict = {}\n",
    "    \n",
    "    # For each entry in the dictionary\n",
    "    for entry in conceptVals:\n",
    "        size = len(conceptVals[entry])\n",
    "        \n",
    "        conceptVals[entry] = [s for s in conceptVals[entry] if s in [-0.25,-.5,-0.75,-1,0,0.25,0.5,0.75,1.0]]\n",
    "        if not conceptVals[entry]:\n",
    "            continue;\n",
    "        # Get all variants (Weights)\n",
    "        indices_25 = [i for i, x in enumerate(conceptVals[entry]) if x == 0.25]\n",
    "        indices_50 = [i for i, x in enumerate(conceptVals[entry]) if x == 0.50]\n",
    "        indices_75 = [i for i, x in enumerate(conceptVals[entry]) if x == 0.75]\n",
    "        indices_n25 = [i for i, x in enumerate(conceptVals[entry]) if x == -0.25]\n",
    "        indices_n50 = [i for i, x in enumerate(conceptVals[entry]) if x == -0.5]\n",
    "        indices_n75 = [i for i, x in enumerate(conceptVals[entry]) if x == -0.75]\n",
    "        \n",
    "        \n",
    "        # Get weight ratios\n",
    "        ratio_25 = len(indices_25)/size\n",
    "        ratio_50 = len(indices_50)/size\n",
    "        ratio_75 = len(indices_75)/size\n",
    "        ratio_n25 = len(indices_n25)/size\n",
    "        ratio_n50 = len(indices_n50)/size\n",
    "        ratio_n75 = len(indices_n75)/size\n",
    "               \n",
    "        # feed into membership (this part may or may not be necessary)\n",
    "        \n",
    "        # combine membership values into one \n",
    "        # combining the rules on positives\n",
    "        r25 = mamdani_rule(r_input, p_tri25, ratio_25)\n",
    "        r50 = mamdani_rule(r_input, p_tri50, ratio_50)\n",
    "        r75 = mamdani_rule(r_input, p_tri75, ratio_75)\n",
    "        R_25_50 = np.fmax(r25,r50)\n",
    "        R_pComb = np.fmax(R_25_50, r75)\n",
    "        \n",
    "        # combining the rules on negatives\n",
    "        rn25 = mamdani_rule(r_input, n_tri25, ratio_n25)\n",
    "        rn50 = mamdani_rule(r_input, n_tri50, ratio_n50)\n",
    "        rn75 = mamdani_rule(r_input, n_tri75, ratio_n75)\n",
    "\n",
    "        Rn_25_50 = np.fmax(rn25, rn50)\n",
    "        R_nComb = np.fmax(Rn_25_50, rn75)\n",
    "        \n",
    "        # combine both\n",
    "        R_Combined = np.fmax(R_pComb, R_nComb)\n",
    "\n",
    "        # save into a agg dictionary \n",
    "        ruleTriggerByInput=R_Combined[len(R_Combined)-2]\n",
    "\n",
    "        aggDict[entry] = fuzz.defuzz(univ, ruleTriggerByInput, 'centroid')\n",
    "        \n",
    "    return aggDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "createIndividualFCMList\n",
    "parameters: graphlist: list of mental models to be made into fcms\n",
    "    scenario: Which scenario to initialize the mental models for\n",
    "returns: a list of fcms\n",
    "creates a list of individual FCMs from an input list of individual maps and a scenario\n",
    "'''\n",
    "def createIndividualFCMList(graphList, scenario):\n",
    "    individualFCMList = []\n",
    "    \n",
    "    for graph in graphList:\n",
    "        individualGraph = []\n",
    "        individualGraph.append(graph)\n",
    "        \n",
    "        individualFCM = makeFuzzy(individualGraph)\n",
    "        individualFCM = scenario(individualFCM)\n",
    "        \n",
    "        individualFCMList.append(individualFCM)\n",
    "        \n",
    "    return individualFCMList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "transFunct\n",
    "parameters: x: a value between 0 and 1\n",
    "returns: tanh(a) to keep value in range of 0 and 1\n",
    "transfer function for the FCM simulation\n",
    "'''\n",
    "def transFunct(x):\n",
    "    return 1/(1+math.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Validation Scenarios\n",
    "To validate  our model(s), we create two scenarios to ensure that they give realistic predictions compared to what would be expected. For this method of validation, we used extreme scenarios where there is no doubt in what the outcome should be. Thus, we will consider two scenarios that should clearly (i) cause an increase in the total pike population of legal size or (ii) cause a decrease in the total pike population of legal size. These scenarios are developed as stories in the following paragraphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Scenario 1: Increase in Pike Population\n",
    "We will initialize a model with conditions optimal for the breeding of pike, such that we should see an increase in the amount of pike. For this we will leave the amount of pike the same as we described originally. The study by Casselman~\\cite{casselman1996habitat} shows us which features are vital to an increase in pike population. As such we will have high amounts of refuge and spawning areas (spawning grounds = refuge = .6). We want the water to be clean and not deep, thus we want lower values for the turbidity (turbidity = .1). Along with that, we increase the amount of plants for food and cover (algae = emergent plants = submergent plants = zooplankton = .5). We will keep the nutrients of plants low (plant nutrients = .2). Furthermore, we will set angling pressure low to avoid possible overfishing (angling pressure = .2). All remaining values are left same as in our previously mentioned initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "validation1\n",
    "parameters: fcmnew: an initialized fcm for our ecological problem\n",
    "returns: an fcm set for our validation scenario\n",
    "validate that FCM gives expected results. In this one we attempt to increase the amount \n",
    "of pike by making a scenario apt for improvement\n",
    "'''\n",
    "def validation1(fcmNew):\n",
    "    fcmNew.set_value('spawning grounds', 0.6)\n",
    "    fcmNew.set_value('angling pressure', .2)\n",
    "    fcmNew.set_value('hiding places, refuges', 0.6)\n",
    "    fcmNew.set_value('pike population (adult, over the legal size limit)', .8)\n",
    "    fcmNew.set_value('stocked pike (adult, over the legal size limit)', .21)\n",
    "    fcmNew.set_value('stocked pike, young fish (under the legal size limit)', .25)\n",
    "    fcmNew.set_value('baitfish, prey fish', .15)\n",
    "    fcmNew.set_value('other predatory fish', .15)\n",
    "    fcmNew.set_value('algae', .5)\n",
    "    fcmNew.set_value('depth of a body of water', .3)\n",
    "    fcmNew.set_value('wild pike, young fish (under the legal size limit)', .5)\n",
    "    fcmNew.set_value('emergent riparian plants (eg reeds and other bank vegetation)', 0.5)\n",
    "    fcmNew.set_value('benthic invertebrates (snails, crustaceans etc)', 0.25)\n",
    "    fcmNew.set_value('zooplankton', .5)\n",
    "    fcmNew.set_value('submerged aquatic plants', .5)\n",
    "    fcmNew.set_value('cormorant', 0.1)\n",
    "    fcmNew.set_value('plant nutrients', 0.18)\n",
    "    fcmNew.set_value('turbidity of water', .1)\n",
    "    fcmNew.set_value('surface area of a body of water', 0.75)\n",
    "\n",
    "    return fcmNew;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Scenario 2: Decrease in Pike Population\n",
    "We will initialize a model with poor conditions for pike to thrive. This can be done by following the opposite logic that was used earlier. Thus, we will make it difficult for the pike to reproduce (spawning grounds = refuge = .1). The water will be dirty and deep with a large amount of predators (cormorant = predatory fish = turbidity = .8 ). The plant life and easily preyed which means there is a small amount of plant life (algae = emergent plants = submergent plants = zooplankton = invertebrates =.1). Finally, the lake will be reliant on stocked pike of fishing size (stocked adult pike = .6) with all other types of pike being in normal parameters from our original initialization. Since this lake is reliant on fishing there will be a large amount of anglers fishing (angling pressure = .9). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "validation2\n",
    "parameters: fcmnew: an initialized fcm for our ecological problem\n",
    "returns: an fcm set for our validation scenario\n",
    "validate that FCM gives expected results. In this one we attempt to decrease the amount \n",
    "of pike by making a scenario that should decrease the amount of pike with a bad habitat and lots of fishing\n",
    "'''\n",
    "def validation2(fcmNew):\n",
    "    fcmNew.set_value('spawning grounds', 0.1)\n",
    "    fcmNew.set_value('angling pressure', .9)\n",
    "    fcmNew.set_value('hiding places, refuges', 0.1)\n",
    "    fcmNew.set_value('pike population (adult, over the legal size limit)', .8)\n",
    "    fcmNew.set_value('stocked pike (adult, over the legal size limit)', .21)\n",
    "    fcmNew.set_value('stocked pike, young fish (under the legal size limit)', .25)\n",
    "    fcmNew.set_value('baitfish, prey fish', .15)\n",
    "    fcmNew.set_value('other predatory fish', .8)\n",
    "    fcmNew.set_value('algae', .1)\n",
    "    fcmNew.set_value('depth of a body of water', .3)\n",
    "    fcmNew.set_value('wild pike, young fish (under the legal size limit)', .5)\n",
    "    fcmNew.set_value('emergent riparian plants (eg reeds and other bank vegetation)', 0.1)\n",
    "    fcmNew.set_value('benthic invertebrates (snails, crustaceans etc)', 0.1)\n",
    "    fcmNew.set_value('zooplankton', .1)\n",
    "    fcmNew.set_value('submerged aquatic plants', .1)\n",
    "    fcmNew.set_value('cormorant', 0.8)\n",
    "    fcmNew.set_value('plant nutrients', 0.18)\n",
    "    fcmNew.set_value('turbidity of water', .8)\n",
    "    fcmNew.set_value('surface area of a body of water', 0.75)\n",
    "\n",
    "    return fcmNew;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we run our validation scenarios on the aggreate expert map. The expert maps were chosen for validation because we expect them to be the most accurate. We see that in our first validation scenario performs successfully, meanng the amount of harvestable pike increased as would be expected. However, the aggregate map did not properly decrease the number of pike when angling pressure increases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "validateScenario1\n",
    "parameters: graphList: the list of graphs of the group map to validate(aggregate fcm)\n",
    "returns: Null\n",
    "Desc: will run scenario on the aggrgate map for the graph list submitted. WIll print out success of failure\n",
    "'''\n",
    "def validateScenario1(graphList):\n",
    "    expertAggValid = makeFuzzy(graphList)\n",
    "    expertAggValid = validation1(expertAggValid)\n",
    "    pikeBase = .8\n",
    "    simFcm = simulation(expertAggValid)\n",
    "    simFcm.steps(100)\n",
    "    simFcm.stabilize('pike population (adult, over the legal size limit)', .001)\n",
    "    simFcm.changeTransferFunction(transFunct)\n",
    "    fcmResults = simFcm.run()\n",
    "    if fcmResults[-1]['pike population (adult, over the legal size limit)'] > pikeBase:\n",
    "        print \"Aggregate success on validation 1\"\n",
    "    else:\n",
    "        print 'Validation Failure'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "validateScenario2\n",
    "parameters: graphList: the list of graphs of the group map to validate(aggregate fcm)\n",
    "returns: Null\n",
    "Desc: will run scenario on the aggrgate map for the graph list submitted. WIll print out success of failure\n",
    "'''\n",
    "def validateScenario2(graphList):\n",
    "    expertAggValid2 = makeFuzzy(graphList)\n",
    "    expertAggValid2 = validation2(expertAggValid2)\n",
    "    pikeBase = .8\n",
    "    simFcm = simulation(expertAggValid2)\n",
    "    simFcm.steps(100)\n",
    "    simFcm.stabilize('pike population (adult, over the legal size limit)', .001)\n",
    "    simFcm.changeTransferFunction(transFunct)\n",
    "    fcmResults = simFcm.run()\n",
    "    if fcmResults[-1]['pike population (adult, over the legal size limit)'] < pikeBase:\n",
    "        print \"Aggregate success on validation 2\"\n",
    "    else:\n",
    "        print 'Validation Failure'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the aggregate maps did not succeed the validation scenarios, we want to see the percentage of expert maps that pass our validation scenarios. To do this, we simulate each expert FCM and compare the final amount of harvestable pike with the initial amount. We then calculate the percentage of expert maps that pass the validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "validationPercent1\n",
    "parameters: graphList: graphList for a group to see amount that pass validation 1\n",
    "return NUll\n",
    "will print out the percentage of models that pass the first validation scenario\n",
    "'''\n",
    "def validationPercent1(graphList):\n",
    "    expertValidation = createIndividualFCMList(graphList, validation1)\n",
    "    pikeBase = .8\n",
    "    countSuccess = 0\n",
    "    for element in expertValidation:\n",
    "        simFcm = simulation(element)\n",
    "        simFcm.steps(100)\n",
    "        simFcm.stabilize('pike population (adult, over the legal size limit)', .001)\n",
    "        simFcm.changeTransferFunction(transFunct)\n",
    "        fcmResults = simFcm.run()\n",
    "        if fcmResults[-1]['pike population (adult, over the legal size limit)'] > pikeBase:\n",
    "            countSuccess += 1\n",
    "    print \"Success percentage for simulation is: \",countSuccess/len(expertValidation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "validationPercent2\n",
    "parameters: graphList: graphList for a group to see amount that pass validation 1\n",
    "return NUll\n",
    "will print out the percentage of models that pass the second validation scenario\n",
    "'''\n",
    "def validationPercent2(graphList):\n",
    "    expertValidation2 = createIndividualFCMList(graphList, validation2)\n",
    "    pikeBase = .8\n",
    "    countSuccess = 0\n",
    "    for element in expertValidation2:\n",
    "        simFcm = simulation(element)\n",
    "        simFcm.steps(100)\n",
    "        simFcm.stabilize('pike population (adult, over the legal size limit)', .001)\n",
    "        simFcm.changeTransferFunction(transFunct)\n",
    "        fcmResults = simFcm.run()\n",
    "        if fcmResults[-1]['pike population (adult, over the legal size limit)'] < pikeBase:\n",
    "            countSuccess += 1\n",
    "    print \"Success percentage for simulation is: \",countSuccess/len(expertValidation2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above results we can see the expert maps have a bias towards increasing the pike population. With 82% of the maps increasing the pike population when expected but on 23% succeeding when we create a scenario that would decrease the total harvestable pike population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Create Scenarios\n",
    "Here we create 4 scenarios for our ecological problem. These scenarios all derive from the same baseline as defined in the paper. For each scenario we will either increase or decrease the concept of interest by 0.25. Our four scenarios are\n",
    "1) Increasing Juvenile Stocking\n",
    "2) Decrease Angling Pressure\n",
    "3) Increase the spawning habitat\n",
    "4) Increasing Refuge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Set initial concept values for the FCM\n",
    "Scenario 1\n",
    "increase jevenile stocking\n",
    "'''\n",
    "def initialiseFCM1(fcmNew):\n",
    "    fcmNew.set_value('spawning grounds', 0.35)\n",
    "    fcmNew.set_value('angling pressure', 0.75)\n",
    "    fcmNew.set_value('hiding places, refuges', 0.25)\n",
    "    fcmNew.set_value('pike population (adult, over the legal size limit)', .31)\n",
    "    fcmNew.set_value('stocked pike (adult, over the legal size limit)', .21)\n",
    "    fcmNew.set_value('stocked pike, young fish (under the legal size limit)', .5)\n",
    "    fcmNew.set_value('baitfish, prey fish', .15)\n",
    "    fcmNew.set_value('other predatory fish', .15)\n",
    "    fcmNew.set_value('algae', .18)\n",
    "    fcmNew.set_value('depth of a body of water', .3)\n",
    "    fcmNew.set_value('wild pike, young fish (under the legal size limit)', .5)\n",
    "    fcmNew.set_value('emergent riparian plants (eg reeds and other bank vegetation)', 0.45)\n",
    "    fcmNew.set_value('benthic invertebrates (snails, crustaceans etc)', 0.25)\n",
    "    fcmNew.set_value('zooplankton', .27)\n",
    "    fcmNew.set_value('submerged aquatic plants', .45)\n",
    "    fcmNew.set_value('cormorant', 0.1)\n",
    "    fcmNew.set_value('plant nutrients', 0.18)\n",
    "    fcmNew.set_value('turbidity of water', .2)\n",
    "    fcmNew.set_value('surface area of a body of water', 0.75)\n",
    "\n",
    "    return fcmNew;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Set initial concept values for the FCM\n",
    "Scenario 2\n",
    "decrease angling pressure\n",
    "'''\n",
    "def initialiseFCM2(fcmNew):\n",
    "    fcmNew.set_value('spawning grounds', 0.35)\n",
    "    fcmNew.set_value('angling pressure', .5)\n",
    "    fcmNew.set_value('hiding places, refuges', 0.25)\n",
    "    fcmNew.set_value('pike population (adult, over the legal size limit)', .31)\n",
    "    fcmNew.set_value('stocked pike (adult, over the legal size limit)', .21)\n",
    "    fcmNew.set_value('stocked pike, young fish (under the legal size limit)', .25)\n",
    "    fcmNew.set_value('baitfish, prey fish', .15)\n",
    "    fcmNew.set_value('other predatory fish', .15)\n",
    "    fcmNew.set_value('algae', .18)\n",
    "    fcmNew.set_value('depth of a body of water', .3)\n",
    "    fcmNew.set_value('wild pike, young fish (under the legal size limit)', .5)\n",
    "    fcmNew.set_value('emergent riparian plants (eg reeds and other bank vegetation)', 0.45)\n",
    "    fcmNew.set_value('benthic invertebrates (snails, crustaceans etc)', 0.25)\n",
    "    fcmNew.set_value('zooplankton', .27)\n",
    "    fcmNew.set_value('submerged aquatic plants', .45)\n",
    "    fcmNew.set_value('cormorant', 0.1)\n",
    "    fcmNew.set_value('plant nutrients', 0.18)\n",
    "    fcmNew.set_value('turbidity of water', .2)\n",
    "    fcmNew.set_value('surface area of a body of water', 0.75)\n",
    "\n",
    "    return fcmNew;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Set initial concept values for the FCM\n",
    "Scenario 3\n",
    "increase spawning habitat\n",
    "'''\n",
    "def initialiseFCM3(fcmNew):\n",
    "    fcmNew.set_value('spawning grounds', 0.6)\n",
    "    fcmNew.set_value('angling pressure', 0.75)\n",
    "    fcmNew.set_value('hiding places, refuges', 0.25)\n",
    "    fcmNew.set_value('pike population (adult, over the legal size limit)', .31)\n",
    "    fcmNew.set_value('stocked pike (adult, over the legal size limit)', .21)\n",
    "    fcmNew.set_value('stocked pike, young fish (under the legal size limit)', .25)\n",
    "    fcmNew.set_value('baitfish, prey fish', .15)\n",
    "    fcmNew.set_value('other predatory fish', .15)\n",
    "    fcmNew.set_value('algae', .18)\n",
    "    fcmNew.set_value('depth of a body of water', .3)\n",
    "    fcmNew.set_value('wild pike, young fish (under the legal size limit)', .5)\n",
    "    fcmNew.set_value('emergent riparian plants (eg reeds and other bank vegetation)', 0.45)\n",
    "    fcmNew.set_value('benthic invertebrates (snails, crustaceans etc)', 0.25)\n",
    "    fcmNew.set_value('zooplankton', .27)\n",
    "    fcmNew.set_value('submerged aquatic plants', .45)\n",
    "    fcmNew.set_value('cormorant', 0.1)\n",
    "    fcmNew.set_value('plant nutrients', 0.18)\n",
    "    fcmNew.set_value('turbidity of water', .2)\n",
    "    fcmNew.set_value('surface area of a body of water', 0.75)\n",
    "\n",
    "    return fcmNew;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Set initial concept values for the FCM\n",
    "Scenario 4\n",
    "increase refuge\n",
    "'''\n",
    "def initialiseFCM4(fcmNew):\n",
    "    fcmNew.set_value('spawning grounds', 0.35)\n",
    "    fcmNew.set_value('angling pressure', 0.75)\n",
    "    fcmNew.set_value('hiding places, refuges', 0.5)\n",
    "    fcmNew.set_value('pike population (adult, over the legal size limit)', .31)\n",
    "    fcmNew.set_value('stocked pike (adult, over the legal size limit)', .21)\n",
    "    fcmNew.set_value('stocked pike, young fish (under the legal size limit)', .25)\n",
    "    fcmNew.set_value('baitfish, prey fish', .15)\n",
    "    fcmNew.set_value('other predatory fish', .15)\n",
    "    fcmNew.set_value('algae', .18)\n",
    "    fcmNew.set_value('depth of a body of water', .3)\n",
    "    fcmNew.set_value('wild pike, young fish (under the legal size limit)', .5)\n",
    "    fcmNew.set_value('emergent riparian plants (eg reeds and other bank vegetation)', 0.45)\n",
    "    fcmNew.set_value('benthic invertebrates (snails, crustaceans etc)', 0.25)\n",
    "    fcmNew.set_value('zooplankton', .27)\n",
    "    fcmNew.set_value('submerged aquatic plants', .45)\n",
    "    fcmNew.set_value('cormorant', 0.1)\n",
    "    fcmNew.set_value('plant nutrients', 0.18)\n",
    "    fcmNew.set_value('turbidity of water', .2)\n",
    "    fcmNew.set_value('surface area of a body of water', 0.75)\n",
    "\n",
    "    return fcmNew;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Initialize FCMs\n",
    "We initialize every FCM in a list for a scenario. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "initializeFCMs\n",
    "parameter: graphList is a list of all the different mental models\n",
    "returns: a list of the aggregate fcms for each scenario presented above\n",
    "Details: This function takes in a list of all the mental models. It them creates the aggregate fuzzy map for\n",
    "the graphs and initializes that for each scenario\n",
    "\n",
    "examples:\n",
    "anglerGroup = initializeFCMs(anglerGraphList)\n",
    "'''\n",
    "def initializeFCMs(graphList):\n",
    "    FCM1C = makeFuzzy(graphList)\n",
    "    FCM1C = initialiseFCM1(FCM1C)\n",
    "    FCM2C = makeFuzzy(graphList)\n",
    "    FCM2C = initialiseFCM2(FCM2C)\n",
    "    FCM3C = makeFuzzy(graphList)\n",
    "    FCM3C = initialiseFCM3(FCM3C)\n",
    "    FCM4C = makeFuzzy(graphList)\n",
    "    FCM4C = initialiseFCM4(FCM4C)\n",
    "    \n",
    "    Group = []\n",
    "    Group.append(FCM1C)\n",
    "    Group.append(FCM2C)\n",
    "    Group.append(FCM3C)\n",
    "    Group.append(FCM4C)\n",
    "    \n",
    "    return Group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Correlating Rankings\n",
    "The final goal of our study is to correlate the ranking of the simulation outcome of each concept with the ranking of concepts centralities. For that we first implement several centralities through the networkx library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "numEdgesFCM\n",
    "parameters: fcm: an fcm\n",
    "returns: number of edges in fcm\n",
    "'''\n",
    "def numEdgesFCM(fcm):\n",
    "    return nx.number_of_edges(fcm._fcm_graph)\n",
    "'''\n",
    "densityFCM\n",
    "parameters: fcm: an fcm\n",
    "returns: denisty of the fcm\n",
    "'''\n",
    "def densityFCM(fcm):\n",
    "    return nx.density(fcm._fcm_graph)\n",
    "'''\n",
    "betweennessFCM\n",
    "parameters: fcm: an fcm, concept: a concept to measure in fcm\n",
    "returns: the betweenness centrality of the conept\n",
    "'''\n",
    "def betweennessFCM(fcm, concept):\n",
    "    btwn = nx.betweenness_centrality(fcm._fcm_graph)\n",
    "    return btwn[concept]\n",
    "'''\n",
    "closenessFCM\n",
    "parameters: fcm: an fcm, concept to measure in fcm\n",
    "returns: closeness centrality of the concept\n",
    "'''\n",
    "def closenessFCM(fcm,concept):\n",
    "    csns = nx.closeness_centrality(fcm._fcm_graph)\n",
    "    return csns[concept]\n",
    "'''\n",
    "eigenVectorFCM\n",
    "parameters: fcm: an fcm, concept to measure in fcm\n",
    "returns: the eigenVector centrality of the concept\n",
    "'''\n",
    "def eigenvectorFCM(fcm,concept):\n",
    "    eign = nx.eigenvector_centrality(fcm._fcm_graph)\n",
    "    return eign[concept]\n",
    "'''\n",
    "katzFCM\n",
    "parameters: fcm: an fcm, concept to measure in fcm\n",
    "returns: the katz centrality of the concept\n",
    "'''\n",
    "def katzFCM(fcm,concept):\n",
    "    katz = nx.katz_centrality(fcm._fcm_graph)\n",
    "    return katz[concept]\n",
    "'''\n",
    "degreeFCM\n",
    "parameters: fcm: an fcm, concept to measure in fcm\n",
    "returns: the degree centrality of the concept\n",
    "'''\n",
    "def degreeFCM(fcm,concept):\n",
    "    deg = nx.degree_centrality(fcm._fcm_graph)\n",
    "    return deg[concept]\n",
    "'''\n",
    "LoadFCM\n",
    "parameters: fcm: an fcm, concept to measure in fcm\n",
    "returns: the load centrality of the conept\n",
    "'''\n",
    "def loadCFCM(fcm,concept):\n",
    "    load = nx.load_centrality(fcm._fcm_graph)\n",
    "    return load[concept]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a functon that will compute the centrality of the fcms we will create and rank the concepts from most central to least central. With the centralities we are focusing on higher values mean more central."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "rankCentrality\n",
    "parameters: fcm: a fuzzy cognitive map\n",
    "            func: a function for centrality ranking\n",
    "returns: A list of tuples (concept name, rank)\n",
    "This function computes the deisred type of centrality for each concept in the FCM. Once the rank is decided, \n",
    "the list is sorted from most central to least central we give the concept its numerical rank in the centrality order,\n",
    "most central being 1 and least central being 19\n",
    "'''\n",
    "def rankCentrality(fcm, func):\n",
    "    rankList = []\n",
    "    concepts = []\n",
    "    #get concepts. concepts returns a dictionary of the concepts and their values. We only care about the key\n",
    "    for concept in fcm.concepts():\n",
    "        concepts.append(concept)\n",
    "        \n",
    "    #for each tuple compute the concepts centrality and assign it to the tuple\n",
    "    for concept in concepts:\n",
    "        rankList.append((concept,func(fcm,concept)))\n",
    "        \n",
    "    toRank = sorted(rankList, key = lambda x: x[1],reverse=True)\n",
    "    rankedList = []\n",
    "    rank = 1\n",
    "    for element in toRank:\n",
    "        rankedList.append((element[0],rank))\n",
    "        rank = rank + 1\n",
    "    return rankedList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want to be able to correlate these centrality rankings with the rankings of the final output values of the simulation for the concepts. To do this we write the function rankOutput which will take in the fcm list and all information needed to perform the simulation and return the ordered results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "rankOutput\n",
    "arguments: fcm: an fcm object to simulate\n",
    "    stableConcept: the concept to stabilize on. can be made to be taken as a list but presently on stabilizes on a singular value\n",
    "    maxSteps: maximum number of steps for the simulation\n",
    "    Stabilizing threshold: the amount of change the the concept must change less than to stop\n",
    "    transfunct: transfer function to keep the values in range\n",
    "Returns: a sorted list of tuples (concept name,rank) ranking the tuples from highest final value to lowest\n",
    "\n",
    "'''\n",
    "def rankOutput(fcm, stableConcept, maxSteps, stabilizingThreshold,transfunct):\n",
    "    rankedResults = [] #list for final rankings\n",
    "    \n",
    "    #declare simulation\n",
    "    simFcm = simulation(fcm)\n",
    "    simFcm.steps(maxSteps)\n",
    "    simFcm.stabilize(stableConcept, stabilizingThreshold)\n",
    "    simFcm.changeTransferFunction(transfunct)\n",
    "    #save results as a list of dictionary. final results are in the last element\n",
    "    fcmResults = simFcm.run()\n",
    "    \n",
    "    for key in fcmResults[-1]:\n",
    "        rankedResults.append((key,fcmResults[-1][key]))\n",
    "        \n",
    "        \n",
    "    toRank = sorted(rankedResults, key = lambda x: x[1],reverse=True)\n",
    "    rankedList = []\n",
    "    rank = 1\n",
    "    for element in toRank:\n",
    "        rankedList.append((element[0],rank))\n",
    "        rank = rank + 1\n",
    "    return rankedList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation\n",
    "To avoid outliers we use the robust regression method of theil-sen. We fit a regressor to to correlate between the rank of a concepts centrality and its simulation output. Since we are measuring the fit of a regressor we are measuring the $R^2$. Thus it will not be the traditional $[-1.1]$ for correlation. We measure correlation by fit, so $1$ is a perfect fit which means a string correlation which could be positive or negative. Since this is $R^2$ instead of correlation though values can go lower han negaitve 1. since any value below zero just means the fit gets arbitrarly worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "correlate\n",
    "parameters: r: the ranked list of tuples (name, rank) of centralities for the fcm\n",
    "        s: the ranked list of the concepts outputs for an fcm as a list of tuples (name,rank)\n",
    "returns: corr: Fit of the regression between the two ranking\n",
    "        newR: r prepped for plotting\n",
    "        newS: s prepped for plotting\n",
    "        y_pred: predicted value for the regresion for plotting\n",
    "desc: this function uses the theil-sen regressor to fir a regression to the ranking between the concepts centrality and its \n",
    "final conept value. It will give us the fit and its predictoin\n",
    "\n",
    "example:\n",
    "betCent = rankCentrality(expertGroup[0],betweennessFCM)\n",
    "expertSort = rankOutput(expertGroup[0],'pike population (adult, over the legal size limit)',100,.001,transFunct)\n",
    "degreeCorr,R,s,y_pred = correlate(degCent,expertSort)\n",
    "'''\n",
    "def correlate(r, s):\n",
    "    #can have ransac estimator run as well for further comparison\n",
    "    estimators = [('Theil-Sen', TheilSenRegressor(random_state=None))]#,('RANSAC', RANSACRegressor(random_state=None))]\n",
    "    newR = []\n",
    "    newS = []\n",
    "    #put just the values in ranked order since r and s are tuples os form (string,double)\n",
    "    for element in r:\n",
    "        newR.append(element[1])\n",
    "        for element2 in s:\n",
    "            if element2[0]==element[0]:\n",
    "                newS.append(element2[1])\n",
    "    #use theil-sen to get correlation\n",
    "    r = np.array(newR)\n",
    "    s = np.array(newS)\n",
    "   \n",
    "    line_r = np.array([0.0, 1.0])\n",
    "    R = r[:, np.newaxis]\n",
    "\n",
    "    for name, estimator in estimators:\n",
    "        fit = estimator.fit(R, s)\n",
    "        y_pred = fit.predict(line_r.reshape(2, 1))\n",
    "    corr = fit.score(R,s,None)\n",
    "    \n",
    "    return corr,newR,newS,y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "plot_theil\n",
    "parameters: R: the ranked list from correlate\n",
    "            s: ranked list from correlated\n",
    "            y_pred: the predicted fit from correlate\n",
    "            ran: the range to display. 20 will show all concepts\n",
    "returns: Null\n",
    "Will plot the rankings from centrality and output and the regressor fit to them by theil sen\n",
    "\n",
    "example(extended from correlate):\n",
    "degreeCorr,R,s,y_pred = correlate(degCent,expertSort)\n",
    "plot_theil(R,s,y_pred)\n",
    "'''\n",
    "def plot_theil(R,s,y_pred,ran=[0,20]):\n",
    "    plt.scatter(R,s,color='indigo',marker='x',s = 150)\n",
    "    #plt.title(title)\n",
    "    plt.xlabel(\"Centrality\")\n",
    "    plt.ylabel(\"Simulation Output\")\n",
    "    plt.ylim([0,20])\n",
    "    plt.xlim([0,20])\n",
    "    plt.plot(ran,y_pred,linewidth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "GenerateCorrelations\n",
    "parameters: fcmList: list of fcms for correlation\n",
    "            stableCOncept: concept which to stabilize on for simulation\n",
    "            maxSteps: max number of steps for the fcm\n",
    "            stabilizing threshold: Value of epsiln. amount of change to stop simulation\n",
    "            transFunct: tansfer function for the simulation\n",
    "            centrality list: list of functions for centality measurements\n",
    "            centralityNames: names of centralities used, in same order as centrality list\n",
    "            title: name for plot\n",
    "returns: Null\n",
    "desc: This function will run the desired fcms using the passed parameters and get the ranking of the concept final values.\n",
    "It will then get the centrality rankings for each desired centrality. WIth this the correlate function will be called \n",
    "and we will use the first returned value which is the R^2 fit of the reggression and create a heatmap to\n",
    "visulize the correlation between centrality and output value\n",
    "\n",
    "example:\n",
    "GenerateCorrelations(expertGroup, 'pike population (adult, over the legal size limit)',100,.001,transFunct, [closenessFCM,degreeFCM,loadCFCM,betweennessFCM,katzFCM],[\"Closeness\",\"Degree\",\"Load\",\"Betweenness\",\"Katz\"],\"Experts\")\n",
    "'''\n",
    "def GenerateCorrelations(fcmList,stableConcept, maxSteps, stabilizingThreshold,transfunct, centralityList,centralityNames,title):\n",
    "    #first we will run the simulation to get the rankOutput rankings. The list are different scenarios so must run all\n",
    "    \n",
    "    sortRank = [] #list of sorted ranked outputs for simulation\n",
    "    for fcm in fcmList:\n",
    "        sortRank.append(rankOutput(fcm, stableConcept, maxSteps, stabilizingThreshold, transfunct)) \n",
    "       \n",
    "    \n",
    "    #now we need the centrality rankings. Must investigate betweeness and katz and load before use\n",
    "    #since we don't care about weihts, the graph is the same throughout\n",
    "    centralityRankList = []\n",
    "    for element in centralityList:\n",
    "        centralityRankList.append(rankCentrality(fcmList[0], element))\n",
    "    \n",
    "    \n",
    "    \n",
    "    #we now have the centrality rankings and the sorted rankings. we now need a correlation for each\n",
    "    w = len(fcmList) #number of scnearios\n",
    "    h = len(centralityList) #number of centrality metrics\n",
    "    shape = (w,h)\n",
    "    matrixForCorrelation = np.zeros(shape)\n",
    "    \n",
    "    i = 0#row\n",
    "    \n",
    "    for element in sortRank:\n",
    "        for j in range(0,h):\n",
    "            matrixForCorrelation[i][j] = correlate(centralityRankList[j],element)\n",
    "        \n",
    "        i = i+1\n",
    "    \n",
    "    sns.heatmap(matrixForCorrelation,cmap=\"RdBu\",annot = True,linewidths = .5,linecolor='white',xticklabels=centralityNames,yticklabels=[\"scenario 1\",\"scenario 2\",\"scenario 3\",\"scenario 4\"],vmin = -1.0,vmax = 1.0).set_title(title)  \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "individualFCMs\n",
    "parameters: anglerGraphList: graphList for angler mental models\n",
    "            waterManagerGraphList: graphList for water manager mental models\n",
    "            clubManagerGraphList: graph list for cum manager mental models\n",
    "            expertGraphList: graph list for expert mental models\n",
    "            nonExpertGraphList: graph list for nonExpert mental models\n",
    "            scenario: scenario initialization function \n",
    "return: a list of fcms for each group in the same order as the parameters\n",
    "desc: will initialize each individuals mental model into an fcm for simulaiton and individuals instead of aggregates\n",
    "\n",
    "example:\n",
    "anglerFCMList, waterManagerFCMList,clubManagerFCMList,expertFCMList,nonExpertFCMList = individualFCMs(anglerGraphList,waterManagerGraphList,clubManagerGraphList,expertGraphList,nonExpertGraphList,initialiseFCM2)\n",
    "'''\n",
    "def individualFCMs(anglerGraphList,waterManagerGraphList,clubManagerGraphList,expertGraphList,nonExpertGraphList,scenario):\n",
    "#we also want to study the individual metrics for scenario 1\n",
    "    anglerFCMList = createIndividualFCMList(anglerGraphList, scenario)\n",
    "    waterManagerFCMList = createIndividualFCMList(waterManagerGraphList, scenario)\n",
    "    clubManagerFCMList = createIndividualFCMList(clubManagerGraphList, scenario)\n",
    "    expertFCMList = createIndividualFCMList(expertGraphList, scenario)\n",
    "    nonExpertFCMList = createIndividualFCMList(nonExpertGraphList, scenario)\n",
    "\n",
    "    return anglerFCMList, waterManagerFCMList,clubManagerFCMList,expertFCMList,nonExpertFCMList\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "FCMListCorr\n",
    "parameters: FCMList: list of fcms for a group\n",
    "            centralityFunc: list of the centrality functions\n",
    "            centralityNames: names of the centrality functions in thesame order\n",
    "            stableConcept: concept to stabilize on\n",
    "            maxSteps: number of steps to simulate at max\n",
    "            stableThreshold: Epsilon, minimum change needed to stop simulation\n",
    "            title: title of plots\n",
    "returns: Null\n",
    "desc: will run the list of fcms for a group and compute the mean and standard deviation of the correlation between\n",
    "centrality and the concepts final output. The mean and standard deviation are then plotted on a box plot\n",
    "\n",
    "example:\n",
    "FCMListCorr(anglerFCMList,[closenessFCM,degreeFCM,loadCFCM,betweennessFCM,katzFCM],[\"Closeness\",\"Degree\",\"Load\",\"Betweenness\",\"Katz\"],'spawning grounds',100,.001,transFunct,\"Anglers\")\n",
    "\n",
    "'''\n",
    "#get the mean rank of the fit between centrality metrics and simulation outcomes. \n",
    "#will extend this to create a box plot\n",
    "#When calling this function plce the centrality names and functions in the same order\n",
    "def FCMListCorr(FCMList,centralityFunc,centralityNames,stableConcept,maxSteps,stableThreshold,transFunct,title):\n",
    "    count = 0 #index to print proper name\n",
    "    plotData = []\n",
    "    for centrality in centralityFunc:\n",
    "        correlateList = [] #list that holds the correlated values. Is emptied after each centrality\n",
    "        for fcm in FCMList:\n",
    "            correlateList.append(correlate(rankCentrality(fcm,centrality),rankOutput(fcm,stableConcept,maxSteps,stableThreshold,transFunct)))\n",
    "            \n",
    "        print centralityNames[count]    \n",
    "        print \"The average is: \", np.mean(correlateList)\n",
    "        print \"The standard deviation is: \", np.std(correlateList)\n",
    "        count += 1\n",
    "        plotData.append(correlateList)\n",
    "        \n",
    "    # Create a figure instance\n",
    "    fig = plt.figure(1, figsize=(9, 6))\n",
    "\n",
    "    # Create an axes instance\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    # Create the boxplot\n",
    "    bp = ax.boxplot(plotData)\n",
    "    #modify plot view\n",
    "    \n",
    "    ax.set_xticklabels(centralityNames)\n",
    "    ax.get_xaxis().tick_bottom()\n",
    "    ax.set_ylim([-1.0,1.0])\n",
    "    plt.title(title)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
